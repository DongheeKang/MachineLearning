{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_style_transfer_with_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNq9gnCvhEVpmI52ij1Mch3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DongheeKang/MachineLearning/blob/master/Image_style_transfer_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Style Transfer with CNN\n",
        "\n",
        "* Image Style Transfer Using Convolutional Neural Networks (CVPR 2016)]\n",
        "* based on the image optimization\n",
        "* ML Framework: PyTorch\n",
        "* Images : style and original"
      ],
      "metadata": {
        "id": "nuL33Aezl24I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import Image as display_image"
      ],
      "metadata": {
        "id": "OM822XNbn4IY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Normalization\n",
        "\n",
        "* Instance normalization calculates mean and std\n",
        "* Feature Map (N: batch size, C: channel size, H: height, W: width)\n",
        "* epsilon to avoid infinite division\n",
        "\n",
        "## Adaptive Instance Normalization\n",
        "\n",
        "* Implementation of AdaIN\n",
        "* AdaIN allows to transfer content feature with combining style feature"
      ],
      "metadata": {
        "id": "OAaDfbWUqM_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "\n",
        "def adaptive_instance_normalization(content_feat, style_feat):\n",
        "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\n",
        "\n",
        "    # normalization with mean and std\n",
        "    normalized_feat = (content_feat - content_mean.expand(size)) / content_std.expand(size)\n",
        "    # calculate statistics of style feature after normalization\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)"
      ],
      "metadata": {
        "id": "HGAe5Xg1n9tg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and Decoder \n",
        "\n",
        "* Encoder: VGG type network, extracting features \n",
        "* Decoder: similar with Encoder but resolution (W x H) of image will be increased\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNjj3Sr9oOaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n",
        "* In VGG, max pooling are done with 4 layers, In style transfer ReLU4_1 is used, it means that downsampling are performed upto 3th layers. "
      ],
      "metadata": {
        "id": "9dh52H2DVNo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "vgg = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, (1, 1)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(3, 64, (3, 3)),\n",
        "    nn.ReLU(), # relu1-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(), # relu1-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 128, (3, 3)),\n",
        "    nn.ReLU(), # relu2-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(), # relu2-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 256, (3, 3)),\n",
        "    nn.ReLU(), # relu3-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(), # relu3-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(), # relu3-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(), # relu3-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu4-1, this is the last layer used\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu4-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu4-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu4-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu5-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu5-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(), # relu5-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU() # relu5-4\n",
        ")"
      ],
      "metadata": {
        "id": "YL8Tdje_oTxi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder\n",
        "\n",
        "* Reverse network from encorder\n",
        "* 3 upsampling layers"
      ],
      "metadata": {
        "id": "32QubrskoXbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "decoder = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 3, (3, 3)),\n",
        ")"
      ],
      "metadata": {
        "id": "i1eMaD1GoX9e"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to mount Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EIZjhVNWYSt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained model"
      ],
      "metadata": {
        "id": "Awtibw37pKXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -alh\n",
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-uDvbRNXsVN",
        "outputId": "ae8e613a-86e8-4031-c3e8-c080557b2710"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 104248\n",
            "drwxr-xr-x 1 root root      4096 Jul 29 20:12 .\n",
            "drwxr-xr-x 1 root root      4096 Jul 29 18:57 ..\n",
            "drwxr-xr-x 4 root root      4096 Jul 28 13:39 .config\n",
            "-rw-r--r-- 1 root root   1048576 Jul 29 19:25 decoder.pth\n",
            "drwxr-xr-x 2 root root      4096 Jul 29 20:12 .ipynb_checkpoints\n",
            "drwxr-xr-x 1 root root      4096 Jul 28 13:40 sample_data\n",
            "-rw-r--r-- 1 root root 105679010 Jul 29 20:19 vgg_normalised.pth\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.eval()\n",
        "vgg.eval()\n",
        "\n",
        "# for Colab, file upload is necessary\n",
        "vgg_path = './vgg_normalised.pth'\n",
        "decoder_path = './decoder.pth'\n",
        "\n",
        "decoder.load_state_dict(torch.load(decoder_path))\n",
        "vgg.load_state_dict(torch.load(vgg_path))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "vgg = nn.Sequential(*list(vgg.children())[:31]) # upto ReLU4_1"
      ],
      "metadata": {
        "id": "RwUOeUrXpOjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaIN Style Transfer\n",
        "\n",
        "* A network with encoder and decoder\n",
        "* encoder extracts feature map -> AdaIN -> Style transfer\n",
        "* decoder receive feature and create a new image"
      ],
      "metadata": {
        "id": "rEVMYbXQpSKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Net, self).__init__()\n",
        "        enc_layers = list(encoder.children())\n",
        "        self.enc_1 = nn.Sequential(*enc_layers[:4]) # input -> relu1_1\n",
        "        self.enc_2 = nn.Sequential(*enc_layers[4:11]) # relu1_1 -> relu2_1\n",
        "        self.enc_3 = nn.Sequential(*enc_layers[11:18]) # relu2_1 -> relu3_1\n",
        "        self.enc_4 = nn.Sequential(*enc_layers[18:31]) # relu3_1 -> relu4_1\n",
        "        self.decoder = decoder\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        # fix the encoder\n",
        "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
        "            for param in getattr(self, name).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    # extract relu4_1 from input image\n",
        "    def encode(self, input):\n",
        "        for i in range(4):\n",
        "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
        "        return input\n",
        "\n",
        "    # content loss\n",
        "    def calc_content_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    # style loss\n",
        "    def calc_style_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + self.mse_loss(input_std, target_std)\n",
        "\n",
        "    def forward(self, content, style, alpha=1.0):\n",
        "        # set a weight between content or style\n",
        "        # 0 means content, 1 means style \n",
        "        assert 0 <= alpha <= 1 \n",
        "        style_feats = self.encode_with_intermediate(style)\n",
        "        content_feat = self.encode(content)\n",
        "        t = adain(content_feat, style_feats[-1])\n",
        "        t = alpha * t + (1 - alpha) * content_feat\n",
        "\n",
        "        g_t = self.decoder(t) # result image\n",
        "        g_t_feats = self.encode_with_intermediate(g_t)\n",
        "\n",
        "        # loss calculation \n",
        "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
        "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
        "        for i in range(1, 4):\n",
        "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
        "        return loss_c, loss_s"
      ],
      "metadata": {
        "id": "uieKprPvpV29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# content \n",
        "display_image('./images/content_img_1.jpg')\n",
        "# style\n",
        "display_image('./images/style_img_1.jpg')"
      ],
      "metadata": {
        "id": "DhbtBI6Ipb55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Style Transfer Function"
      ],
      "metadata": {
        "id": "pSytYK0_pf4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def style_transfer(vgg, decoder, content, style, alpha=1.0):\n",
        "    assert (0.0 <= alpha <= 1.0)\n",
        "    content_f = vgg(content)\n",
        "    style_f = vgg(style)\n",
        "    feat = adaptive_instance_normalization(content_f, style_f)\n",
        "    feat = feat * alpha + content_f * (1 - alpha)\n",
        "    return decoder(feat)"
      ],
      "metadata": {
        "id": "20RAOoTHpgOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre-processing"
      ],
      "metadata": {
        "id": "oV7Oa6QtpkES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_transform(size=512):\n",
        "    transform_list = []\n",
        "    if size != 0:\n",
        "        transform_list.append(transforms.Resize(size))\n",
        "    transform_list.append(transforms.ToTensor())\n",
        "    transform = transforms.Compose(transform_list)\n",
        "    return transform\n",
        "\n",
        "content_tf = test_transform()\n",
        "style_tf = test_transform()"
      ],
      "metadata": {
        "id": "qBoyS7AkpmfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Style Transfer\n",
        "\n"
      ],
      "metadata": {
        "id": "RDZTtzmdpxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = './images/content_img_1.jpg'\n",
        "style_path = './images/style_img_1.jpg'\n",
        "\n",
        "content = content_tf(Image.open(str(content_path)))\n",
        "style = style_tf(Image.open(str(style_path)))\n",
        "\n",
        "style = style.to(device).unsqueeze(0)\n",
        "content = content.to(device).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = style_transfer(vgg, decoder, content, style, alpha=1.0)\n",
        "output = output.cpu()\n",
        "\n",
        "save_image(output, 'output.png')"
      ],
      "metadata": {
        "id": "vs9aFh-JpxwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image('./output.png')"
      ],
      "metadata": {
        "id": "E3XT45iHp2FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = './images/content_img_2.jpg'\n",
        "style_path = './images/style_img_2.jpg'\n",
        "\n",
        "content = content_tf(Image.open(str(content_path)))\n",
        "style = style_tf(Image.open(str(style_path)))\n",
        "\n",
        "style = style.to(device).unsqueeze(0)\n",
        "content = content.to(device).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    output = style_transfer(vgg, decoder, content, style, alpha=1.0)\n",
        "output = output.cpu()\n",
        "\n",
        "save_image(output, './output.png')"
      ],
      "metadata": {
        "id": "RcGIrCe0p5JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image('./output.png')"
      ],
      "metadata": {
        "id": "jE2K6dUfp86S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}