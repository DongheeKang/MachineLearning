{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Training\n",
    "\n",
    "Train a convolutional neural network on multiple GPU with TensorFlow 2.0+.\n",
    "* Apple M1 GPU\n",
    "* After installing tensorflow-metal GPU should be available\n",
    "* CIFAR10 dataset \n",
    "* 60000 32x32 colour images in 10 classe (6000 images per class)\n",
    "* There are 50000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset parameters.\n",
    "num_classes = 10 # total classes (0-9 digits).\n",
    "num_gpus = 4\n",
    "\n",
    "# Training parameters.\n",
    "learning_rate = 0.001\n",
    "training_steps = 1000\n",
    "# Split batch size equally between GPUs.\n",
    "# Note: Reduce batch size if you encounter OOM Errors.\n",
    "batch_size = 1024 * num_gpus\n",
    "display_step = 20\n",
    "\n",
    "# Network parameters.\n",
    "conv1_filters = 64 # number of filters for 1st conv layer.\n",
    "conv2_filters = 128 # number of filters for 2nd conv layer.\n",
    "conv3_filters = 256 # number of filters for 2nd conv layer.\n",
    "fc1_units = 2048 # number of neurons for 1st fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST data.\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "y_train, y_test = np.reshape(y_train, (-1)), np.reshape(y_test, (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:17:51.073425: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-02 10:17:51.073518: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(batch_size * 10).batch(batch_size).prefetch(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Convolution Layer with 64 filters and a kernel size of 3.\n",
    "        self.conv1_1 = layers.Conv2D(conv1_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        self.conv1_2 = layers.Conv2D(conv1_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2. \n",
    "        self.maxpool1 = layers.MaxPool2D(2, strides=2)\n",
    "\n",
    "        # Convolution Layer with 128 filters and a kernel size of 3.\n",
    "        self.conv2_1 = layers.Conv2D(conv2_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        self.conv2_2 = layers.Conv2D(conv2_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        self.conv2_3 = layers.Conv2D(conv2_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2. \n",
    "        self.maxpool2 = layers.MaxPool2D(2, strides=2)\n",
    "\n",
    "        # Convolution Layer with 256 filters and a kernel size of 3.\n",
    "        self.conv3_1 = layers.Conv2D(conv3_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        self.conv3_2 = layers.Conv2D(conv3_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        self.conv3_3 = layers.Conv2D(conv3_filters, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer.\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # Fully connected layer.\n",
    "        self.fc1 = layers.Dense(1024, activation=tf.nn.relu)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied).\n",
    "        self.dropout = layers.Dropout(rate=0.5)\n",
    "\n",
    "        # Output layer, class prediction.\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    # Set forward pass.\n",
    "    @tf.function\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.conv2_3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=is_training)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits.\n",
    "@tf.function\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "@tf.function\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def backprop(batch_x, batch_y, trainable_variables):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = conv_net(batch_x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        # Compute gradients.\n",
    "        gradients = g.gradient(loss, trainable_variables)\n",
    "    return gradients\n",
    "\n",
    "# Build the function to average the gradients.\n",
    "@tf.function\n",
    "def average_gradients(tower_grads):\n",
    "    avg_grads = []\n",
    "    for tgrads in zip(*tower_grads):\n",
    "        grads = []\n",
    "        for g in tgrads:\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            grads.append(expanded_g)\n",
    "        \n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        \n",
    "        avg_grads.append(grad)\n",
    "        \n",
    "    return avg_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # Build convnet.\n",
    "    conv_net = ConvNet()\n",
    "    # Stochastic gradient descent optimizer.\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process.\n",
    "def run_optimization(x, y):\n",
    "    # Save gradients for all GPUs.\n",
    "    tower_grads = []\n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = conv_net.trainable_variables\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        for i in range(num_gpus):\n",
    "            # Split data between GPUs.\n",
    "            gpu_batch_size = int(batch_size/num_gpus)\n",
    "            batch_x = x[i * gpu_batch_size: (i+1) * gpu_batch_size]\n",
    "            batch_y = y[i * gpu_batch_size: (i+1) * gpu_batch_size]\n",
    "            \n",
    "            # Build the neural net on each GPU.\n",
    "            with tf.device('/gpu:%i' % i):\n",
    "                grad = backprop(batch_x, batch_y, trainable_variables)\n",
    "                tower_grads.append(grad)\n",
    "                    \n",
    "                # Last GPU Average gradients from all GPUs.\n",
    "                if i == num_gpus - 1:\n",
    "                    gradients = average_gradients(tower_grads)\n",
    "\n",
    "        # Update vars following gradients.\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not require any more\n",
    "\n",
    "#from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "#mlcompute.set_mlc_device(device_name='gpu')\n",
    "\n",
    "#from tensorflow.python.framework.ops import disable_eager_execution\n",
    "#disable_eager_execution()\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU available, then go ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:17:51.990455: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-02 10:17:51.991231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-02 10:17:52.070225: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-02 10:17:52.631629: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-02 10:17:54.362027: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-08-02 10:17:54.466505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss: 2.302616, accuracy: 0.086670, speed: 146300.117509 examples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 10:17:58.118681: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20, loss: 2.298103, accuracy: 0.156982, speed: 1060.400099 examples/sec\n",
      "step: 40, loss: 2.250992, accuracy: 0.252930, speed: 1012.317403 examples/sec\n",
      "step: 60, loss: 2.178898, accuracy: 0.348633, speed: 864.832048 examples/sec\n",
      "step: 80, loss: 2.132300, accuracy: 0.408447, speed: 772.781365 examples/sec\n",
      "step: 100, loss: 2.108729, accuracy: 0.439697, speed: 971.874094 examples/sec\n",
      "step: 120, loss: 2.063211, accuracy: 0.481934, speed: 1047.969151 examples/sec\n",
      "step: 140, loss: 2.039878, accuracy: 0.523438, speed: 1063.273636 examples/sec\n",
      "step: 160, loss: 2.019253, accuracy: 0.538086, speed: 1064.420695 examples/sec\n",
      "step: 180, loss: 2.024312, accuracy: 0.553955, speed: 1063.684320 examples/sec\n",
      "step: 200, loss: 1.935198, accuracy: 0.618164, speed: 1061.610512 examples/sec\n",
      "step: 220, loss: 1.935233, accuracy: 0.624756, speed: 1061.910597 examples/sec\n",
      "step: 240, loss: 1.887085, accuracy: 0.665283, speed: 1063.000559 examples/sec\n",
      "step: 260, loss: 1.999961, accuracy: 0.587891, speed: 1044.258989 examples/sec\n",
      "step: 280, loss: 1.856589, accuracy: 0.692383, speed: 1033.292656 examples/sec\n",
      "step: 300, loss: 1.829204, accuracy: 0.714844, speed: 1030.823306 examples/sec\n",
      "step: 320, loss: 1.798380, accuracy: 0.747559, speed: 1020.817548 examples/sec\n",
      "step: 340, loss: 1.742670, accuracy: 0.788574, speed: 1024.404808 examples/sec\n",
      "step: 360, loss: 1.746708, accuracy: 0.797852, speed: 1052.784523 examples/sec\n",
      "step: 380, loss: 1.701387, accuracy: 0.823975, speed: 1040.548808 examples/sec\n",
      "step: 400, loss: 1.645143, accuracy: 0.872314, speed: 1032.240451 examples/sec\n",
      "step: 420, loss: 1.612091, accuracy: 0.914307, speed: 1016.646884 examples/sec\n",
      "step: 440, loss: 1.582162, accuracy: 0.927979, speed: 989.298489 examples/sec\n",
      "step: 460, loss: 1.606986, accuracy: 0.925049, speed: 1003.094458 examples/sec\n",
      "step: 480, loss: 1.522480, accuracy: 0.974609, speed: 1033.206840 examples/sec\n",
      "step: 500, loss: 1.508865, accuracy: 0.985107, speed: 1054.612945 examples/sec\n",
      "step: 520, loss: 1.499425, accuracy: 0.981445, speed: 1045.992057 examples/sec\n",
      "step: 540, loss: 1.483493, accuracy: 0.993408, speed: 1015.811467 examples/sec\n",
      "step: 560, loss: 1.476212, accuracy: 0.997803, speed: 1028.198932 examples/sec\n",
      "step: 580, loss: 1.469972, accuracy: 0.998779, speed: 1037.149069 examples/sec\n",
      "step: 600, loss: 1.468744, accuracy: 0.998291, speed: 1028.247086 examples/sec\n",
      "step: 620, loss: 1.465977, accuracy: 1.000000, speed: 1057.933826 examples/sec\n",
      "step: 640, loss: 1.466032, accuracy: 0.999268, speed: 1056.751216 examples/sec\n",
      "step: 660, loss: 1.465601, accuracy: 0.999756, speed: 1010.337322 examples/sec\n",
      "step: 680, loss: 1.464202, accuracy: 0.999756, speed: 1026.476053 examples/sec\n",
      "step: 700, loss: 1.465566, accuracy: 0.999268, speed: 1019.914213 examples/sec\n",
      "step: 720, loss: 1.465464, accuracy: 0.999512, speed: 1024.159104 examples/sec\n",
      "step: 740, loss: 1.468570, accuracy: 0.998779, speed: 1008.312410 examples/sec\n",
      "step: 760, loss: 1.463194, accuracy: 1.000000, speed: 1015.571720 examples/sec\n",
      "step: 780, loss: 1.462587, accuracy: 1.000000, speed: 1038.162468 examples/sec\n",
      "step: 800, loss: 1.462042, accuracy: 0.999756, speed: 1028.627991 examples/sec\n",
      "step: 820, loss: 1.462014, accuracy: 1.000000, speed: 1020.843600 examples/sec\n",
      "step: 840, loss: 1.461800, accuracy: 1.000000, speed: 1016.141771 examples/sec\n",
      "step: 860, loss: 1.462592, accuracy: 0.999756, speed: 1046.167729 examples/sec\n",
      "step: 880, loss: 1.462120, accuracy: 0.999756, speed: 1057.553584 examples/sec\n",
      "step: 900, loss: 1.461745, accuracy: 1.000000, speed: 1057.899628 examples/sec\n",
      "step: 920, loss: 1.461863, accuracy: 0.999756, speed: 1059.285062 examples/sec\n",
      "step: 940, loss: 1.461725, accuracy: 1.000000, speed: 1056.348163 examples/sec\n",
      "step: 960, loss: 1.461557, accuracy: 1.000000, speed: 1037.478429 examples/sec\n",
      "step: 980, loss: 1.461670, accuracy: 1.000000, speed: 1028.070194 examples/sec\n",
      "step: 1000, loss: 1.461414, accuracy: 1.000000, speed: 1049.898017 examples/sec\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "ts = time.time()\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0 or step == 1:\n",
    "        dt = time.time() - ts\n",
    "        speed = batch_size * display_step / dt\n",
    "        pred = conv_net(batch_x)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f, speed: %f examples/sec\" % (step, loss, acc, speed))\n",
    "        ts = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
