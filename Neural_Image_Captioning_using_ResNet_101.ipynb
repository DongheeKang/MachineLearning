{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Image_Captioning_using_ResNet_101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOG4Eak/rjVnXONUXok7X/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82b0f88433f44a3fb6b31303f879484c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31da8217e5234d329c44731030430096",
              "IPY_MODEL_93f00b2b9adb44eba3ab7ee5a29dab3d",
              "IPY_MODEL_ff73892349e547398a0dcba817794cd8"
            ],
            "layout": "IPY_MODEL_2aa4feca8d85424e85c07b84f1085bbb"
          }
        },
        "31da8217e5234d329c44731030430096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9095dcd43e304ecc8adf0bfa926e66a2",
            "placeholder": "​",
            "style": "IPY_MODEL_8b30f998f32b4d80876efef9b45a23d4",
            "value": "100%"
          }
        },
        "93f00b2b9adb44eba3ab7ee5a29dab3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cdbbbdc11240c688f1f02129438ccb",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71781e81916a4171acf201e27ab0516f",
            "value": 178793939
          }
        },
        "ff73892349e547398a0dcba817794cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_300a6eebbc3a4794af677ace3c9fe689",
            "placeholder": "​",
            "style": "IPY_MODEL_7d190dd1491d4a2fb21774692a541bf3",
            "value": " 171M/171M [00:01&lt;00:00, 102MB/s]"
          }
        },
        "2aa4feca8d85424e85c07b84f1085bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9095dcd43e304ecc8adf0bfa926e66a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b30f998f32b4d80876efef9b45a23d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98cdbbbdc11240c688f1f02129438ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71781e81916a4171acf201e27ab0516f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "300a6eebbc3a4794af677ace3c9fe689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d190dd1491d4a2fb21774692a541bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DongheeKang/MachineLearning/blob/master/Neural_Image_Captioning_using_ResNet_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFX9Tg7XS_o8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Neural Image Caption Generator\n",
        "\n",
        "* A Neural Image Caption Generator (CVPR 2015)\n",
        "* CNN :ResNet-101\n",
        "* Dataset Flickr8k\n",
        "* Framework: PyTorch\n",
        "* Purpose: BLEU score\n",
        "* https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning"
      ],
      "metadata": {
        "id": "KB4dWYpzTCJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 -O Flickr8k_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cULjwoIiTVZx",
        "outputId": "c09af1f1-ecce-4c70-a4f2-591b137ffa23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-30 08:51:23--  https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1\n",
            "Resolving postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)... 13.107.136.9, 13.107.138.9\n",
            "Connecting to postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)|13.107.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/dongbinna_postech_ac_kr/Documents/Research/datasets/Flickr8k_dataset.zip?ga=1 [following]\n",
            "--2022-07-30 08:51:24--  https://postechackr-my.sharepoint.com/personal/dongbinna_postech_ac_kr/Documents/Research/datasets/Flickr8k_dataset.zip?ga=1\n",
            "Reusing existing connection to postechackr-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1112971163 (1.0G) [application/x-zip-compressed]\n",
            "Saving to: ‘Flickr8k_dataset.zip’\n",
            "\n",
            "Flickr8k_dataset.zi 100%[===================>]   1.04G  52.2MB/s    in 22s     \n",
            "\n",
            "2022-07-30 08:51:46 (49.0 MB/s) - ‘Flickr8k_dataset.zip’ saved [1112971163/1112971163]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip Flickr8k_dataset.zip -d ./Flickr8k_dataset"
      ],
      "metadata": {
        "id": "jdcqhsjYTXxk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score library\n",
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTaUMtmsTbCF",
        "outputId": "03ebc04f-bf2c-4eef-9ed1-d7ea0d10f9b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 36.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.0\n",
            "    Uninstalling torchtext-0.13.0:\n",
            "      Successfully uninstalled torchtext-0.13.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Resie"
      ],
      "metadata": {
        "id": "kudoNfjBTx2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "image_dir = \"./Flickr8k_dataset/Images\"\n",
        "train_image_dir = \"./resized_train/images\" \n",
        "val_image_dir = \"./resized_val/images\" \n",
        "test_image_dir = \"./resized_test/images\"\n",
        "size = [256, 256]\n",
        "\n",
        "\n",
        "def resize_image(image, size):\n",
        "    return image.resize(size, Image.ANTIALIAS)\n",
        "\n",
        "if not os.path.exists(train_image_dir):\n",
        "    os.makedirs(train_image_dir)\n",
        "if not os.path.exists(val_image_dir):\n",
        "    os.makedirs(val_image_dir)\n",
        "if not os.path.exists(test_image_dir):\n",
        "    os.makedirs(test_image_dir)\n",
        "\n",
        "images = sorted(os.listdir(image_dir))\n",
        "num_images = len(images)\n",
        "num_train_images = 6000 # trainning\n",
        "num_val_images = 1000 # validation\n",
        "\n",
        "for i, image in enumerate(images):\n",
        "    if (i + 1) <= num_train_images:\n",
        "        output_dir = train_image_dir\n",
        "    elif (i + 1) <= num_train_images + num_val_images:\n",
        "        output_dir = val_image_dir\n",
        "    else:\n",
        "        output_dir = test_image_dir\n",
        "    with open(os.path.join(image_dir, image), 'rb+') as f:\n",
        "        with Image.open(f) as img:\n",
        "            img = resize_image(img, size)\n",
        "            img.save(os.path.join(output_dir, image), img.format)\n",
        "    if (i + 1) % 500 == 0:\n",
        "        print(f\"[{i + 1}/{num_images}] Resized the images and saved into '{output_dir}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqLBd_GUT4uD",
        "outputId": "129c5ac6-1438-418f-8b67-b899a32d2c7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500/8091] Resized the images and saved into './resized_train/images'\n",
            "[1000/8091] Resized the images and saved into './resized_train/images'\n",
            "[1500/8091] Resized the images and saved into './resized_train/images'\n",
            "[2000/8091] Resized the images and saved into './resized_train/images'\n",
            "[2500/8091] Resized the images and saved into './resized_train/images'\n",
            "[3000/8091] Resized the images and saved into './resized_train/images'\n",
            "[3500/8091] Resized the images and saved into './resized_train/images'\n",
            "[4000/8091] Resized the images and saved into './resized_train/images'\n",
            "[4500/8091] Resized the images and saved into './resized_train/images'\n",
            "[5000/8091] Resized the images and saved into './resized_train/images'\n",
            "[5500/8091] Resized the images and saved into './resized_train/images'\n",
            "[6000/8091] Resized the images and saved into './resized_train/images'\n",
            "[6500/8091] Resized the images and saved into './resized_val/images'\n",
            "[7000/8091] Resized the images and saved into './resized_val/images'\n",
            "[7500/8091] Resized the images and saved into './resized_test/images'\n",
            "[8000/8091] Resized the images and saved into './resized_test/images'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yklz2bCbUGsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Vocabulary with caption\n",
        "* total number of images in Flickr8k = 8,091\n",
        "* total number of captions = 8091 x 5 = 40,455, since each image contains 5 captions"
      ],
      "metadata": {
        "id": "gG9tVGTLUIXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "caption_path = \"./Flickr8k_dataset/captions.txt\" # caption file\n",
        "vocab_path = \"./vocab.pkl\" # vacabulary\n",
        "word_threshold = 4 \n",
        "train_caption_path = \"./resized_train/captions.txt\" \n",
        "val_caption_path = \"./resized_val/captions.txt\" \n",
        "test_caption_path = \"./resized_test/captions.txt\" \n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "with open(caption_path, \"r\") as f:\n",
        "    lines = sorted(f.readlines()[1:])\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        if (i + 1) <= num_train_images * 5: # 5 captions per image\n",
        "            output_caption = train_caption_path\n",
        "        elif (i + 1) <= (num_train_images + num_val_images) * 5:\n",
        "            output_caption = val_caption_path\n",
        "        else:\n",
        "            output_caption = test_caption_path\n",
        "        index = line.find(\",\")  \n",
        "        caption = line[index + 1:] \n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower()) \n",
        "        counter.update(tokens)\n",
        "        with open(output_caption, \"a\") as output_caption_f:\n",
        "            output_caption_f.write(line)\n",
        "\n",
        "# filter by threshold\n",
        "words = [word for word, cnt in counter.items() if cnt >= word_threshold]\n",
        "\n",
        "# Vocabulary object\n",
        "vocab = Vocabulary()\n",
        "vocab.add_word('<pad>')\n",
        "vocab.add_word('<start>')\n",
        "vocab.add_word('<end>')\n",
        "vocab.add_word('<unk>') # unknown token!\n",
        "\n",
        "# contain all vocabulary\n",
        "for word in words:\n",
        "    vocab.add_word(word)\n",
        "\n",
        "# save Vocabulary file\n",
        "with open(vocab_path, 'wb') as f:\n",
        "    pickle.dump(vocab, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KRurGL0UVBS",
        "outputId": "f4371872-b2ae-4d83-9dc5-d2a4e7e37651"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "!wc -l ./resized_train/captions.txt\n",
        "# validation\n",
        "!wc -l ./resized_val/captions.txt\n",
        "# test\n",
        "!wc -l ./resized_test/captions.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CSrpM7nU_Db",
        "outputId": "f675f6bf-30f1-4a0a-ed91-a7e80f3b2e25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000 ./resized_train/captions.txt\n",
            "5000 ./resized_val/captions.txt\n",
            "5455 ./resized_test/captions.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized dataset"
      ],
      "metadata": {
        "id": "5bu4etWhVHsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "# Flickr8k dataset\n",
        "class Flickr8kDataset(data.Dataset):\n",
        "    def __init__(self, root, captions, vocab, transform=None):\n",
        "        self.root = root \n",
        "        with open(captions, \"r\") as f:\n",
        "             lines = f.readlines()\n",
        "             self.captions = [] \n",
        "             for line in lines: # \n",
        "                index = line.find(\",\")\n",
        "                path = line[:index] \n",
        "                caption = line[index + 1:]\n",
        "                self.captions.append((path, caption))\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    # access caption from the image \n",
        "    def __getitem__(self, index):\n",
        "        vocab = self.vocab\n",
        "        path = self.captions[index][0]\n",
        "        caption = self.captions[index][1]\n",
        "\n",
        "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # tokenized\n",
        "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "        caption = []\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)"
      ],
      "metadata": {
        "id": "-T6XdJ-aVFS6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tuple (image, caption) to batch\n",
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    input\n",
        "    * data: list of tuple (image, caption). \n",
        "        * image: torch tensor of shape (3, 256, 256).\n",
        "        * caption: torch tensor of shape (?); variable length.\n",
        "    output\n",
        "    * images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "    * targets: torch tensor of shape (batch_size, padded_length).\n",
        "    * lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # sort by caption length with descending\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    # stack of list to tensor(images, 3, 256, 256)\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # list of captions to tensor\n",
        "    lengths = [len(caption) for caption in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    # for each caption, fill the token.\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]\n",
        "    return images, targets, lengths\n",
        "\n",
        "def collate_fn_test(data):\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    lengths = [len(caption) for caption in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]\n",
        "    return images, targets, lengths\n",
        "\n",
        "# customized Flickr8k dataset by DataLoader\n",
        "def get_loader(root, captions, vocab, transform, batch_size, shuffle, num_workers, testing):\n",
        "    flickr8k = Flickr8kDataset(root=root, captions=captions, vocab=vocab, transform=transform)\n",
        "    # This will return (images, captions, lengths) for each iteration.\n",
        "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
        "    # captions: a tensor of shape (batch_size, padded_length).\n",
        "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
        "    if not testing:\n",
        "        data_loader = torch.utils.data.DataLoader(dataset=flickr8k, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
        "    else:\n",
        "        data_loader = torch.utils.data.DataLoader(dataset=flickr8k, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn_test)\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "FUH0ygp4Ve9M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the ML model\n",
        "\n",
        "* Encoder and Decoder\n",
        "* use ResNet-101"
      ],
      "metadata": {
        "id": "FvtInwpBWnDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        # ResNet-101\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet101(pretrained=True)\n",
        "        # remove FC layer!!!\n",
        "        modules = list(resnet.children())[:-1] \n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size) # output -> embding\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # feature vectors from images\n",
        "        with torch.no_grad(): # keep first network architecture\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        # set hyper-parameters and layers\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seg_length = max_seq_length\n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        # create caption from feature vectors\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # link image property with embeding\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) # same dimension\n",
        "        hiddens, _ = self.lstm(packed) # find next hidden state\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, features, states=None):\n",
        "        # create caption by searching greedy\n",
        "        sampled_indexes = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seg_length):\n",
        "            hiddens, states = self.lstm(inputs, states) # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1)) # outputs: (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1) # predicted: (batch_size)\n",
        "            sampled_indexes.append(predicted)\n",
        "            inputs = self.embed(predicted) # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1) # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_indexes = torch.stack(sampled_indexes, 1) # sampled_indexes: (batch_size, max_seq_length)\n",
        "        return sampled_indexes"
      ],
      "metadata": {
        "id": "HW0dBVqEWkwa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation"
      ],
      "metadata": {
        "id": "yd69H8vhXhc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_path = \"models/\" \n",
        "crop_size = 224 \n",
        "vocab_path = \"./vocab.pkl\" \n",
        "\n",
        "# model repository\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "# Vocabulary\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "# use pre-trained ResNet\n",
        "train_transform = transforms.Compose([ \n",
        "    transforms.RandomCrop(crop_size),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "val_transform = transforms.Compose([ \n",
        "    transforms.Resize(crop_size), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "test_transform = transforms.Compose([ \n",
        "    transforms.Resize(crop_size), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "\n",
        "# data loader\n",
        "train_data_loader = get_loader(train_image_dir, train_caption_path, vocab, train_transform, batch_size, shuffle=True, num_workers=num_workers, testing=False) \n",
        "val_data_loader = get_loader(val_image_dir, val_caption_path, vocab, val_transform, batch_size, shuffle=False, num_workers=num_workers, testing=False)\n",
        "test_data_loader = get_loader(test_image_dir, test_caption_path, vocab, test_transform, batch_size, shuffle=False, num_workers=num_workers, testing=True)"
      ],
      "metadata": {
        "id": "u0fHlqR3XnQk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper-parameters\n",
        "embed_size = 256 # embedding\n",
        "hidden_size = 512 # LSTM hidden states\n",
        "num_layers = 1 # LSTM layers\n",
        "\n",
        "# encoder and decoder\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "log_step = 20\n",
        "save_step = 1000\n",
        "\n",
        "# loss function(cross entrophy) and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161,
          "referenced_widgets": [
            "82b0f88433f44a3fb6b31303f879484c",
            "31da8217e5234d329c44731030430096",
            "93f00b2b9adb44eba3ab7ee5a29dab3d",
            "ff73892349e547398a0dcba817794cd8",
            "2aa4feca8d85424e85c07b84f1085bbb",
            "9095dcd43e304ecc8adf0bfa926e66a2",
            "8b30f998f32b4d80876efef9b45a23d4",
            "98cdbbbdc11240c688f1f02129438ccb",
            "71781e81916a4171acf201e27ab0516f",
            "300a6eebbc3a4794af677ace3c9fe689",
            "7d190dd1491d4a2fb21774692a541bf3"
          ]
        },
        "id": "NJBIZluqXsmZ",
        "outputId": "7fe9332d-389c-4ff5-aca3-fe1dd2655d3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82b0f88433f44a3fb6b31303f879484c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Let's start!\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # 먼저 학습 진행하기\n",
        "    print(\"[ Training ]\")\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    total_step = len(train_data_loader)\n",
        "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "\n",
        "        # forward and backward\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions, lengths)\n",
        "        loss = criterion(outputs, targets)\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss\n",
        "        total_loss += loss.item()\n",
        "        total_count += images.shape[0]\n",
        "\n",
        "        # monitor\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Average Loss: {:.4f}, Perplexity: {:5.4f}, Elapsed time: {:.4f}s'\n",
        "                  .format(epoch, num_epochs, i, total_step, total_loss / total_count, np.exp(loss.item()), time.time() - start_time))\n",
        "\n",
        "    # save\n",
        "    torch.save(decoder.state_dict(), os.path.join(model_path, f'decoder-{epoch + 1}.ckpt'))\n",
        "    torch.save(encoder.state_dict(), os.path.join(model_path, f'encoder-{epoch + 1}.ckpt'))\n",
        "    print(f\"Model saved: {os.path.join(model_path, f'decoder-{epoch + 1}.ckpt')}\")\n",
        "    print(f\"Model saved: {os.path.join(model_path, f'encoder-{epoch + 1}.ckpt')}\")\n",
        "\n",
        "    # validation\n",
        "    print(\"[ Validation ]\")\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    total_step = len(val_data_loader)\n",
        "    with torch.no_grad():\n",
        "        for i, (images, captions, lengths) in enumerate(val_data_loader):\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "\n",
        "            # forward only\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, captions, lengths)\n",
        "            loss = criterion(outputs, targets)\n",
        "  \n",
        "            # loss\n",
        "            total_loss += loss.item()\n",
        "            total_count += images.shape[0]\n",
        "\n",
        "            # log\n",
        "            if i % log_step == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Average Loss: {:.4f}, Perplexity: {:5.4f}, Elapsed time: {:.4f}s'\n",
        "                      .format(epoch, num_epochs, i, total_step, total_loss / total_count, np.exp(loss.item()), time.time() - start_time))"
      ],
      "metadata": {
        "id": "40Ov_X3pYNqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to resue later on.\n",
        "from google.colab import files\n",
        "\n",
        "files.download('models/encoder-5.ckpt')\n",
        "files.download('models/decoder-5.ckpt')"
      ],
      "metadata": {
        "id": "01npnrYGY0tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when training is already performed, and if you need to test only \n",
        "!git clone https://github.com/DongheeKang/MachineLearning\n",
        "!ls -alh"
      ],
      "metadata": {
        "id": "I9lpk1qmZy8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "Vie7YXAzY2im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize([224, 224], Image.LANCZOS)\n",
        "\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    \n",
        "    return image\n",
        "\n",
        "image_path = \"./resized_test/images/872622575_ba1d3632cc.jpg\" # test image\n",
        "encoder_path = \"./models/NIC_encoder_ResNet101.ckpt\" # path for trained encoder\n",
        "decoder_path = \"./models/NIC_decoder_ResNet101.ckpt\" # path for trained decoder\n",
        "vocab_path = \"./vocab.pkl\" # path for vocabulary wrapper\n",
        "\n",
        "# Model parameters (should be same as paramters in train.py)\n",
        "embed_size = 256 # dimension of word embedding vectors\n",
        "hidden_size = 512 # dimension of lstm hidden states\n",
        "num_layers = 1 # number of layers in lstm"
      ],
      "metadata": {
        "id": "E3Bb8hrQZVBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Load vocabulary wrapper\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "# Build models\n",
        "encoder = EncoderCNN(embed_size).eval() # eval mode (batchnorm uses moving mean/variance)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "# Load the trained model parameters\n",
        "encoder.load_state_dict(torch.load(encoder_path))\n",
        "decoder.load_state_dict(torch.load(decoder_path))\n",
        "\n",
        "# Prepare an image\n",
        "image = load_image(image_path, transform)\n",
        "image_tensor = image.to(device)\n",
        "\n",
        "# Generate an caption from the image\n",
        "feature = encoder(image_tensor)\n",
        "sampled_ids = decoder.sample(feature)\n",
        "sampled_ids = sampled_ids[0].cpu().numpy() # (1, max_seq_length) -> (max_seq_length)\n",
        "\n",
        "# Convert word_ids to words\n",
        "sampled_caption = []\n",
        "for word_id in sampled_ids: \n",
        "    word = vocab.idx2word[word_id]\n",
        "    sampled_caption.append(word)\n",
        "    if word == '<end>':\n",
        "        break\n",
        "sentence = ' '.join(sampled_caption)"
      ],
      "metadata": {
        "id": "syua_JbFalap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# image and created caption \n",
        "image = Image.open(image_path)\n",
        "plt.imshow(np.asarray(image))\n",
        "plt.show()\n",
        "print(sentence)"
      ],
      "metadata": {
        "id": "6vXPyI4daurr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU Score"
      ],
      "metadata": {
        "id": "OmNAhbbKa4BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "answers = []\n",
        "answers_per_image = []\n",
        "\n",
        "total_step = len(test_data_loader)\n",
        "cnt = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, captions, lengths) in enumerate(test_data_loader):\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # forward\n",
        "        features = encoder(images)\n",
        "        sampled_ids_list = decoder.sample(features)\n",
        "\n",
        "        for index in range(len(images)):\n",
        "            sampled_ids = sampled_ids_list[index].cpu().numpy()\n",
        "\n",
        "            # answer sentences\n",
        "            answer = []\n",
        "            for word_id in captions[index]:\n",
        "                word = vocab.idx2word[word_id.item()] \n",
        "                answer.append(word)\n",
        "                if word == '<end>':\n",
        "                    break\n",
        "            answers_per_image.append(answer[1:-1]) # ignore <sos> and <eos>\n",
        "\n",
        "            if (cnt + 1) % 5 == 0: # \n",
        "                answers.append(answers_per_image) \n",
        "                answers_per_image = []\n",
        "\n",
        "                # predicted sentences\n",
        "                prediction = []\n",
        "                for word_id in sampled_ids: \n",
        "                    word = vocab.idx2word[word_id] \n",
        "                    prediction.append(word)\n",
        "                    if word == '<end>':\n",
        "                        break\n",
        "                predictions.append(prediction[1:-1]) # ignore <sos> and <eos>\n",
        "            cnt += 1\n",
        "\n",
        "        if i % log_step == 0:\n",
        "            print(f\"[ Testing ] Batch size: {i}/{total_step}\")\n",
        "\n",
        "print(\"predicted sentense:\", len(predictions))\n",
        "print(\"ansered sentense:\", len(answers))"
      ],
      "metadata": {
        "id": "wNEaPl-3a6q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "print(\"[ answer caption]\")\n",
        "for answer in answers[index]:\n",
        "    print(answer)\n",
        "\n",
        "print(\"[ predicted caption ]\")\n",
        "print(predictions[index])"
      ],
      "metadata": {
        "id": "tPcqqc75bf2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "bleu = bleu_score(predictions, answers, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "print(f'Total BLEU Score = {bleu * 100:.2f}')\n",
        "\n",
        "individual_bleu1_score = bleu_score(predictions, answers, max_n=4, weights=[1, 0, 0, 0])\n",
        "individual_bleu2_score = bleu_score(predictions, answers, max_n=4, weights=[0, 1, 0, 0])\n",
        "individual_bleu3_score = bleu_score(predictions, answers, max_n=4, weights=[0, 0, 1, 0])\n",
        "individual_bleu4_score = bleu_score(predictions, answers, max_n=4, weights=[0, 0, 0, 1])\n",
        "\n",
        "print(f'Individual BLEU1 score = {individual_bleu1_score * 100:.2f}') \n",
        "print(f'Individual BLEU2 score = {individual_bleu2_score * 100:.2f}') \n",
        "print(f'Individual BLEU3 score = {individual_bleu3_score * 100:.2f}') \n",
        "print(f'Individual BLEU4 score = {individual_bleu4_score * 100:.2f}') \n",
        "\n",
        "cumulative_bleu1_score = bleu_score(predictions, answers, max_n=4, weights=[1, 0, 0, 0])\n",
        "cumulative_bleu2_score = bleu_score(predictions, answers, max_n=4, weights=[1/2, 1/2, 0, 0])\n",
        "cumulative_bleu3_score = bleu_score(predictions, answers, max_n=4, weights=[1/3, 1/3, 1/3, 0])\n",
        "cumulative_bleu4_score = bleu_score(predictions, answers, max_n=4, weights=[1/4, 1/4, 1/4, 1/4])\n",
        "\n",
        "print(f'Cumulative BLEU1 score = {cumulative_bleu1_score * 100:.2f}') \n",
        "print(f'Cumulative BLEU2 score = {cumulative_bleu2_score * 100:.2f}') \n",
        "print(f'Cumulative BLEU3 score = {cumulative_bleu3_score * 100:.2f}') \n",
        "print(f'Cumulative BLEU4 score = {cumulative_bleu4_score * 100:.2f}') "
      ],
      "metadata": {
        "id": "Tu4mGlfybnTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}