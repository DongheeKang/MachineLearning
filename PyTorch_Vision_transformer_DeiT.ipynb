{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DongheeKang/MachineLearning/blob/master/PyTorch_Vision_transformer_DeiT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Vision Transformer Model"
      ],
      "metadata": {
        "id": "0Pv3k6IflOmz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xCbXTeSt85u"
      },
      "source": [
        "What is DeiT? \n",
        "\n",
        "CNNs typically require hundreds of millions of images for training to achieve the state-of-the-art SOTA results. DeiT is a vision transformer model that requires a lot less\n",
        "data and computing resources for training to compete with the leading\n",
        "CNNs in performing image classification, which is made possible by two\n",
        "key components of of DeiT:\n",
        "\n",
        "-  Data augmentation that simulates training on a much larger dataset;\n",
        "-  Native distillation that allows the transformer network to learn from\n",
        "   a CNN’s output.\n",
        "\n",
        "DeiT shows that Transformers can be successfully applied to computer\n",
        "vision tasks, with limited access to data and resources.\n",
        "\n",
        "Facebook Data-efficient Image Transformers `DeiT <https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification>`:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAd7Jp7At85t"
      },
      "source": [
        "How to use the DeiT? \n",
        "\n",
        "Vision Transformer models apply the cutting-edge attention-based\n",
        "transformer models, introduced in Natural Language Processing to achieve\n",
        "all kinds of the SOTA results, to Computer Vision tasks. \n",
        "\n",
        "* compare the performance of quantized, optimized and non-quantized, non-optimized\n",
        "models, \n",
        "* show the benefits of applying quantization and optimization\n",
        "to the model along the steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z78jF-9t85v"
      },
      "source": [
        "Classifying Images with DeiT\n",
        "-------------------------------\n",
        "\n",
        "Follow the README at the DeiT repo for detailed information on how to\n",
        "classify images using DeiT, or for a quick test, first install the\n",
        "required packages:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a0FY6d1ft85w"
      },
      "outputs": [],
      "source": [
        "# pip install torch torchvision timm pandas requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ddnxLvt85w"
      },
      "source": [
        "To run in Google Colab, uncomment the following line:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MpXJY0J2t85w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89119ea0-7584-486c-d4cc-d5dd0c9947ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 14.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.6.7\n"
          ]
        }
      ],
      "source": [
        "!pip install timm pandas requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZuEfXsst85x"
      },
      "source": [
        "then run the script below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LKUmUQhct85x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "ec304c5fee4a43428b4bab2126d5f23e",
            "e1d9cd21803b42138d19d807290830f7",
            "0da91b870c5f4deeb25830411c507381",
            "a2c0b16a999e4a5b82e19fdbc1274bdb",
            "51ad8576b27145e89dc4047b79715c66",
            "abd477276fd8400bb245930a4c90e68b",
            "dc18927dfc2b443cae58b0378112d0dd",
            "2d24b01b38124d5cadfdbba0dd061868",
            "98eaee9c82c144758c99085080ed9dc6",
            "c80314408e26426aa58e10bec047c398",
            "b691afdbc0a242df93d3e07b42d3816f"
          ]
        },
        "outputId": "68980b5a-c7f5-4a4d-9efa-e60a5a666dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/330M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec304c5fee4a43428b4bab2126d5f23e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import timm\n",
        "\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(torch.__version__)\n",
        "# should be 1.8.0\n",
        "\n",
        "\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=3),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "])\n",
        "\n",
        "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
        "img = transform(img)[None,]\n",
        "out = model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdM7HYhct85y"
      },
      "source": [
        "The output should be 269, which, according to the ImageNet list of class\n",
        "index to `labels file <https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_, maps to ‘timber\n",
        "wolf, grey wolf, gray wolf, Canis lupus’.\n",
        "\n",
        "Now that we have verified that we can use the DeiT model to classify\n",
        "images, let’s see how to modify the model so it can run on iOS and\n",
        "Android apps.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-8OycwRt85y"
      },
      "source": [
        "Scripting DeiT\n",
        "----------------------\n",
        "To use the model on mobile, we first need to script the\n",
        "model. See the `Script and Optimize recipe <https://pytorch.org/tutorials/recipes/script_optimized.html>`_ for a\n",
        "quick overview. Run the code below to convert the DeiT model used in the\n",
        "previous step to the TorchScript format that can run on mobile.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DaiNXPe2t85y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73a5f05-186b-46cf-f1d6-4eda9851fb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "scripted_model = torch.jit.script(model)\n",
        "scripted_model.save(\"fbdeit_scripted.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBBJFNOCt85z"
      },
      "source": [
        "The scripted model file fbdeit_scripted.pt of size about 346MB is\n",
        "generated.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C45T8cc0t85z"
      },
      "source": [
        "Quantizing DeiT\n",
        "---------------------\n",
        "To reduce the trained model size significantly while\n",
        "keeping the inference accuracy about the same, quantization can be\n",
        "applied to the model. Thanks to the transformer model used in DeiT, we\n",
        "can easily apply dynamic-quantization to the model, because dynamic\n",
        "quantization works best for LSTM and transformer models (see `here <https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-quantization>`_\n",
        "for more details).\n",
        "\n",
        "Now run the code below:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k7sIJNv5t85z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3b3aee-ab11-467f-bc33-288c68738d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:178: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        }
      ],
      "source": [
        "# Use 'fbgemm' for server inference and 'qnnpack' for mobile inference\n",
        "backend = \"fbgemm\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n",
        "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
        "scripted_quantized_model = torch.jit.script(quantized_model)\n",
        "scripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPam3faht85z"
      },
      "source": [
        "This generates the scripted and quantized version of the model\n",
        "fbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of\n",
        "the non-quantized model size of 346MB!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eACllmt85z"
      },
      "source": [
        "You can use the ``scripted_quantized_model`` to generate the same\n",
        "inference result:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AIyZzdgqt85z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0887e5b-4765-4a1f-cba3-131d039d1923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ],
      "source": [
        "out = scripted_quantized_model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())\n",
        "# The same output 269 should be printed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWj9BGMQt85z"
      },
      "source": [
        "Optimizing DeiT\n",
        "---------------------\n",
        "The final step before using the quantized and scripted\n",
        "model on mobile is to optimize it:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6CRMoZ27t85z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
        "optimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaeeeq0St85z"
      },
      "source": [
        "The generated fbdeit_optimized_scripted_quantized.pt file has about the\n",
        "same size as the quantized, scripted, but non-optimized model. The\n",
        "inference result remains the same.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BIOmYNhXt85z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19df80d2-40d9-4a8c-a3a9-cebf2e7903e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1130: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1405.)\n",
            "  return forward_call(*input, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "out = optimized_scripted_quantized_model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())\n",
        "# Again, the same output 269 should be printed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2rg1iWrt850"
      },
      "source": [
        "Using Lite Interpreter\n",
        "------------------------\n",
        "\n",
        "To see how much model size reduction and inference speed up the Lite\n",
        "Interpreter can result in, let’s create the lite version of the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kjcnuHxnt850"
      },
      "outputs": [],
      "source": [
        "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\n",
        "ptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtljOCFft850"
      },
      "source": [
        "Although the lite model size is comparable to the non-lite version, when\n",
        "running the lite version on mobile, the inference speed up is expected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqjEzN9ct850"
      },
      "source": [
        "Comparing Inference Speed\n",
        "---------------------------\n",
        "\n",
        "To see how the inference speed differs for the four models - the\n",
        "original model, the scripted model, the quantized-and-scripted model,\n",
        "the optimized-quantized-and-scripted model - run the code below:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8vqRwLkvt850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c1a00d-8400-4d52-c01e-f4087a93f3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original model: 1837.46ms\n",
            "scripted model: 1688.36ms\n",
            "scripted & quantized model: 555.15ms\n",
            "scripted & quantized & optimized model: 514.35ms\n",
            "lite model: 517.10ms\n"
          ]
        }
      ],
      "source": [
        "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n",
        "    out = model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n",
        "    out = scripted_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n",
        "    out = scripted_quantized_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n",
        "    out = optimized_scripted_quantized_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n",
        "    out = ptl(img)\n",
        "\n",
        "print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n",
        "print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n",
        "print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n",
        "print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n",
        "print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtIc9Hx7t851"
      },
      "source": [
        "The following results summarize the inference time taken by each model\n",
        "and the percentage reduction of each model relative to the original\n",
        "model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RGF7Lsv-t851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "afc17849-8fed-4e8f-ca76-efe54c5ec9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Model Inference Time Reduction\n",
            "0                          original model      1837.46ms        0%\n",
            "1                          scripted model      1688.36ms     8.11%\n",
            "2              scripted & quantized model       555.15ms    69.79%\n",
            "3  scripted & quantized & optimized model       514.35ms    72.01%\n",
            "4                              lite model       517.10ms    71.86%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n        Model                             Inference Time    Reduction\\n0\\toriginal model                             1236.69ms           0%\\n1\\tscripted model                             1226.72ms        0.81%\\n2\\tscripted & quantized model                  593.19ms       52.03%\\n3\\tscripted & quantized & optimized model      598.01ms       51.64%\\n4\\tlite model                                  600.72ms       51.43%\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\n",
        "df = pd.concat([df, pd.DataFrame([\n",
        "    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n",
        "    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
        "    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
        "    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
        "    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n",
        "    columns=['Inference Time', 'Reduction'])], axis=1)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\"\"\"\n",
        "        Model                             Inference Time    Reduction\n",
        "0\toriginal model                             1236.69ms           0%\n",
        "1\tscripted model                             1226.72ms        0.81%\n",
        "2\tscripted & quantized model                  593.19ms       52.03%\n",
        "3\tscripted & quantized & optimized model      598.01ms       51.64%\n",
        "4\tlite model                                  600.72ms       51.43%\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFqcutyUt851"
      },
      "source": [
        "Learn More\n",
        "~~~~~~~~~~~~~~~~~\n",
        "\n",
        "- `Facebook Data-efficient Image Transformers <https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification>`__\n",
        "- `Vision Transformer with ImageNet and MNIST on iOS <https://github.com/pytorch/ios-demo-app/tree/master/ViT4MNIST>`__\n",
        "- `Vision Transformer with ImageNet and MNIST on Android <https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST>`__\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "PyTorch_Vision_transformer_DeiT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec304c5fee4a43428b4bab2126d5f23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1d9cd21803b42138d19d807290830f7",
              "IPY_MODEL_0da91b870c5f4deeb25830411c507381",
              "IPY_MODEL_a2c0b16a999e4a5b82e19fdbc1274bdb"
            ],
            "layout": "IPY_MODEL_51ad8576b27145e89dc4047b79715c66"
          }
        },
        "e1d9cd21803b42138d19d807290830f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abd477276fd8400bb245930a4c90e68b",
            "placeholder": "​",
            "style": "IPY_MODEL_dc18927dfc2b443cae58b0378112d0dd",
            "value": "100%"
          }
        },
        "0da91b870c5f4deeb25830411c507381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d24b01b38124d5cadfdbba0dd061868",
            "max": 346319111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98eaee9c82c144758c99085080ed9dc6",
            "value": 346319111
          }
        },
        "a2c0b16a999e4a5b82e19fdbc1274bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c80314408e26426aa58e10bec047c398",
            "placeholder": "​",
            "style": "IPY_MODEL_b691afdbc0a242df93d3e07b42d3816f",
            "value": " 330M/330M [00:15&lt;00:00, 25.2MB/s]"
          }
        },
        "51ad8576b27145e89dc4047b79715c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abd477276fd8400bb245930a4c90e68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc18927dfc2b443cae58b0378112d0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d24b01b38124d5cadfdbba0dd061868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98eaee9c82c144758c99085080ed9dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c80314408e26426aa58e10bec047c398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b691afdbc0a242df93d3e07b42d3816f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}